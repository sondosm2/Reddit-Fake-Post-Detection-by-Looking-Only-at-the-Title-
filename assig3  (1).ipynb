{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc1e47f6",
   "metadata": {},
   "source": [
    "# Problem Formulation\n",
    "Now a days the world has many social problems, one of those is fake information. we are going to predict if news are fakeor real by lookinh at it's tittle.                                                                                      \n",
    "data contains raw (different forms of words) so it is up to you on what text preprocessing techniques to be applied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba41ba3d",
   "metadata": {},
   "source": [
    "# The input and output\n",
    "The input contains new row after preprocessing (new_text)                         \n",
    "The output represents as y (label feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcfa472",
   "metadata": {},
   "source": [
    "# What data mining function is required?\n",
    "predict function is the process of identifying the unavailable data for new observation based on the previous data and classification function to identify class label of new observation to which it belongs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e779beb3",
   "metadata": {},
   "source": [
    "# What could be the challenges?  \n",
    "The training data given has y_label as 0,1 or 2. The label \"2\" is not the accurate labels as the final output can be 0 or 1 only. I removed those entries with label as 2\n",
    "-how to handle upper case in words with lower case                                                                     \n",
    "-remove stop words                                                                   \n",
    "-make stemming process                                   \n",
    "-remove any html tags                            \n",
    "-remove single letter chars                                                       \n",
    "-convert all whitespaces to single whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b274d0ff",
   "metadata": {},
   "source": [
    "# Why do we remove stop words? \n",
    "stop words are available in abundance in any human language. By removing these words, we remove the low-level information from our text in order to give more focus to the important information.\n",
    "Removal of stop words definitely reduces the dataset size and thus reduces the training time due to the fewer number of tokens involved in the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de647078",
   "metadata": {},
   "source": [
    "# why do we make stemmer?\n",
    " is a technique used to reduce text dimensionality. Stemming is also a type of text normalization that enables you to standardize some words into specific expressions also called stems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f11c94",
   "metadata": {},
   "source": [
    "# What is the impact?\n",
    "analyze the performance of the models when trained on each resulting dataset with respect to f1-score and roc scoring .       \n",
    "After preprocessing for data , i apply classifier models to predict if the news are fake or real.\n",
    "### First classifier is logistic regression \n",
    "with word vectorizer that has F1-score=84 is larger than logistic regression with char vectorizer that has f1-score=69.\n",
    "\n",
    "### The second one is XGboost\n",
    "That has f1-score=0.81 with word vectorizer\n",
    "second i increased the values of hyperparameters. That has f1-score=0.83 with word vectorizer\n",
    "Third with char vectorizer ,it gives f1-score= 0.71\n",
    "### Third one is random forest classifier\n",
    "that has 1-score =81 \n",
    "### The fourth one is KNN classifier \n",
    "that has f1-score =77 \n",
    "### Fifth is decision tree \n",
    "with word vectorizer has f1-score=67 and \n",
    "with char vectorizer has f1-score =65\n",
    "#### As we show the result with character vectorizer is not the best choise but word vectorizer gives larger f1-score than char vectorizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c64601",
   "metadata": {},
   "source": [
    "# What is an ideal solution?\n",
    "ideal best solution is logistic regression classifier , it gives high F1-score with word vectorizer =0.84"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2863ead5",
   "metadata": {},
   "source": [
    "# importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bfb105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk #natural lan tool kit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from mlxtend.feature_selection import ColumnSelector\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1233e33e",
   "metadata": {},
   "source": [
    "# read data\n",
    "The data in this problem is in the text format. Each data entry contains number of words which represents the title of the news. It is a labelled data with label as 1 or 0 which implies if the news is fake or not respectively. Number of words in each data entry varies as the title length changes. So the input to the model is the text data 'text of the news' and output of the model is the prediction whether the give titled news is fake or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85fccb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train=pd.read_csv('xy_train.csv',index_col='id')\n",
    "data_test=pd.read_csv('x_test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a772b88c",
   "metadata": {},
   "source": [
    "# Data exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dea242f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.isnull().sum() #check null value for every feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10b28361",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=data_train.loc[data_train[\"label\"] >= 2] # display how many row with label=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ce1ca48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.value_counts of                                                      text  label\n",
       "id                                                              \n",
       "540454  World's Funniest Joke|鈥淭wo hunters are out in ...      2\n",
       "342238  The Simpsons writer: 'Why I predicted Donald T...      2\n",
       "552146  Kanye's philosophy book could explain why he d...      2\n",
       "549212  Larry David Gave Rob McElhenney This Advice Ab...      2\n",
       "398378  Parents Who Bring Forgotten Lunch to School Se...      2\n",
       "...                                                   ...    ...\n",
       "219497  鈥淭hree tips on getting your beach body that do...      2\n",
       "54937   Disgusting: ISIS Just Released A 2-Star Review...      2\n",
       "505566  I Used To Think All Terrorists Were Muslim. Th...      2\n",
       "288391  He鈥檚 Running: Joe Biden Just Made A Pinterest ...      2\n",
       "99749   John Oliver rips Jeff Sessions and names Trump...      2\n",
       "\n",
       "[232 rows x 2 columns]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9856c72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean noise (label =2) \n",
    "data_train.drop(data_train.index[data_train['label']==2],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e151626",
   "metadata": {},
   "source": [
    "## Cleaning and pre processing\n",
    "NLP deals with text . we must remove html tags ,single letter charachters , stopwords ,punctutaion and make stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "555bf5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Lab\n",
      "[nltk_data]     3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Lab\n",
      "[nltk_data]     3\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stemmer = SnowballStemmer(\"german\")\n",
    "stop_words = set(stopwords.words(\"german\"))\n",
    "#make function to do the process of removing html and single letter charachters ,stopwords ,punctutaion and make stemmer.\n",
    "\n",
    "def clean_text(text, for_embedding=False):\n",
    "    \"\"\" steps:\n",
    "        - remove any html tags (< /br> often found)\n",
    "        - Keep only ASCII + European Chars and whitespace, no digits\n",
    "        - remove single letter chars\n",
    "        - convert all whitespaces (tabs etc.) to single wspace\n",
    "        if not for embedding (but e.g. tdf-idf):\n",
    "        - all lowercase\n",
    "        - remove stopwords, punctuation and stemm\n",
    "    \"\"\"\n",
    "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE) # to remove white space\n",
    "    RE_TAGS = re.compile(r\"<[^>]+>\") # remove html tags\n",
    "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž ]\", re.IGNORECASE) \n",
    "    RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž]\\b\", re.IGNORECASE) # to remove single charachter\n",
    "    if for_embedding:\n",
    "        # Keep punctuation\n",
    "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE)\n",
    "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(RE_TAGS, \" \", text) # replace with white space\n",
    "    text = re.sub(RE_ASCII, \" \", text)\n",
    "    text = re.sub(RE_SINGLECHAR, \" \", text)\n",
    "    text = re.sub(RE_WSPACE, \" \", text)\n",
    "\n",
    "    word_tokens = word_tokenize(text)\n",
    "    words_tokens_lower = [word.lower() for word in word_tokens]\n",
    "\n",
    "    if for_embedding:\n",
    "        # no stemming, lowering and punctuation / stop words removal\n",
    "        words_filtered = word_tokens\n",
    "    else:\n",
    "        words_filtered = [\n",
    "            stemmer.stem(word) for word in words_tokens_lower if word not in stop_words\n",
    "        ]\n",
    "\n",
    "    text_clean = \" \".join(words_filtered)\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b9849c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train[\"new_text\"] = data_train.loc[data_train[\"text\"].str.len() > 0, \"text\"]\n",
    "data_train[\"new_text\"] = data_train[\"new_text\"].map(\n",
    "    lambda x: clean_text(x, for_embedding=False) if isinstance(x, str) else x\n",
    ")\n",
    "data_test[\"new_text\"] = data_test.loc[data_test[\"text\"].str.len() > 0, \"text\"]\n",
    "data_test[\"new_text\"] = data_test[\"new_text\"].map(\n",
    "    lambda x: clean_text(x, for_embedding=False) if isinstance(x, str) else x\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "864eb39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "265723    group of friend began to volunte at homeless s...\n",
       "284269    british prim minist theresa may on nerv attack...\n",
       "207715    goodyear released kit that allows ps to be bro...\n",
       "551106    happy birthday bob bark the pric is right host...\n",
       "8584      obama to nation innocent cops and unarmed youn...\n",
       "                                ...                        \n",
       "70046     finish snip simo yh during the invasion of fin...\n",
       "189377    nigerian princ scam took from kansas year lat ...\n",
       "93486     is it saf to smok marijuana during pregnancy y...\n",
       "140950    julius caesar upon realizing that everyon the ...\n",
       "34509     jeff bridg releasing leeping tap new album des...\n",
       "Name: new_text, Length: 59768, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define x and y \n",
    "x=data_train[\"new_text\"]\n",
    "y=data_train[\"label\"]\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee2d61b",
   "metadata": {},
   "source": [
    "# split data with 80% training and 20% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf1da3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a5e52b",
   "metadata": {},
   "source": [
    "## make two pipelines contain char and word vecrorizer \n",
    "tokenization is made fistly in which to split the sentence into words then calculate the frequency for each word.\n",
    "for the Tf is Term frequency, and IDF is Inverse document frequency. TF-IDF is a method which gives us a numerical weightage of words which reflects how important the particular word is to a document in a corpus. A corpus is a collection of documents.\n",
    "The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams or single words for example, “Medium blog” is a 2-gram (a bigram), (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams. Only applies if analyzer is not callable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29e836c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(max_df=0.3, min_df=10, ngram_range=(1, 2)))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first pipeline for word vectorizer\n",
    "pipe_vect1 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(analyzer=\"word\", max_df=0.3, min_df=10, ngram_range=(1, 2), norm=\"l2\"\n",
    "))])\n",
    "#first pipeline for char vectorizer\n",
    "\n",
    "pipe_vect2=Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(analyzer=\"char\", max_df=0.3, min_df=10, ngram_range=(1, 2), norm=\"l2\"\n",
    "))])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pipe_vect2.fit(x_train,y_train)\n",
    "pipe_vect1.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96ee190a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>new_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stargazer</td>\n",
       "      <td>stargaz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yeah</td>\n",
       "      <td>yeah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PD: Phoenix car thief gets instructions from Y...</td>\n",
       "      <td>pd phoenix car thief get instruction from yout...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>As Trump Accuses Iran, He Has One Problem: His...</td>\n",
       "      <td>as trump accus iran he has one probl his own c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Believers\" - Hezbollah 2011</td>\n",
       "      <td>believ hezbollah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59146</th>\n",
       "      <td>Bicycle taxi drivers of New Delhi</td>\n",
       "      <td>bicycl taxi driv of new delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59147</th>\n",
       "      <td>Trump blows up GOP's formula for winning House...</td>\n",
       "      <td>trump blows up gop formula for winning hous rac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59148</th>\n",
       "      <td>Napoleon returns from his exile on the island ...</td>\n",
       "      <td>napoleon return from his exil on the island of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59149</th>\n",
       "      <td>Deep down he always wanted to be a ballet dancer</td>\n",
       "      <td>deep down he always wanted to be ballet danc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59150</th>\n",
       "      <td>Toddler miraculously survives 6-story fall lan...</td>\n",
       "      <td>toddl miraculously surviv story fall landing o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59151 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0                                             stargazer    \n",
       "1                                                   yeah   \n",
       "2      PD: Phoenix car thief gets instructions from Y...   \n",
       "3      As Trump Accuses Iran, He Has One Problem: His...   \n",
       "4                           \"Believers\" - Hezbollah 2011   \n",
       "...                                                  ...   \n",
       "59146                  Bicycle taxi drivers of New Delhi   \n",
       "59147  Trump blows up GOP's formula for winning House...   \n",
       "59148  Napoleon returns from his exile on the island ...   \n",
       "59149   Deep down he always wanted to be a ballet dancer   \n",
       "59150  Toddler miraculously survives 6-story fall lan...   \n",
       "\n",
       "                                                new_text  \n",
       "0                                                stargaz  \n",
       "1                                                   yeah  \n",
       "2      pd phoenix car thief get instruction from yout...  \n",
       "3      as trump accus iran he has one probl his own c...  \n",
       "4                                       believ hezbollah  \n",
       "...                                                  ...  \n",
       "59146                      bicycl taxi driv of new delhi  \n",
       "59147    trump blows up gop formula for winning hous rac  \n",
       "59148  napoleon return from his exil on the island of...  \n",
       "59149       deep down he always wanted to be ballet danc  \n",
       "59150  toddl miraculously surviv story fall landing o...  \n",
       "\n",
       "[59151 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_testd=data_test.drop(columns=['id'])\n",
    "data_testd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d7ef9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                  stargaz\n",
       "1                                                     yeah\n",
       "2        pd phoenix car thief get instruction from yout...\n",
       "3        as trump accus iran he has one probl his own c...\n",
       "4                                         believ hezbollah\n",
       "                               ...                        \n",
       "59146                        bicycl taxi driv of new delhi\n",
       "59147      trump blows up gop formula for winning hous rac\n",
       "59148    napoleon return from his exil on the island of...\n",
       "59149         deep down he always wanted to be ballet danc\n",
       "59150    toddl miraculously surviv story fall landing o...\n",
       "Name: new_text, Length: 59151, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_=data_testd['new_text']\n",
    "test_ # select row that has new text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5234f5e6",
   "metadata": {},
   "source": [
    "# Grid search with validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edfc6385",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2, x_val, y_train2, y_val = train_test_split(\n",
    "    x_train, y_train, train_size = 0.8, stratify = y_train, random_state = 2022)\n",
    "\n",
    "# Create a list where train data indices are -1 and validation data indices are 0\n",
    "# X_train2 (new training set), X_train\n",
    "split_index = [-1 if x in x_train2.index else 0 for x in x_train.index]\n",
    "\n",
    "# Use the list to create PredefinedSplit\n",
    "pds = PredefinedSplit(test_fold = split_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1caba4",
   "metadata": {},
   "source": [
    "### logistic regression\n",
    "# Trial 1\n",
    "#### i will make logistic regression with word vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed403ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 18 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [       nan 0.87643914        nan 0.88475816 0.88475732 0.8847571\n",
      "        nan 0.79238124        nan 0.862386   0.86238807 0.86238076\n",
      "        nan 0.53753637        nan 0.83682569 0.83693395 0.83682384]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8847581594878194\n",
      "best score {'classifier__C': 1.0, 'classifier__penalty': 'l2', 'classifier__solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "pipe=Pipeline([\n",
    "    ('preprocessor',pipe_vect1),\n",
    "    ('classifier', LogisticRegression()),\n",
    "])\n",
    "C = [1.0, 0.1, 0.01]\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "param_grid = {\n",
    "                  'classifier__C': C,\n",
    "                  'classifier__penalty': penalty,\n",
    "             'classifier__solver':['lbfgs', 'liblinear', 'sag']}\n",
    "\n",
    "# cv=2 means two-fold cross-validation\n",
    "# n_jobs means the cucurrent number of jobs\n",
    "# (on colab since we only have two cpu cores, we set it to 2)\n",
    "grid_search = GridSearchCV(\n",
    "    pipe, param_grid, cv=pds, verbose=1, n_jobs=2, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "# here we still use X_train; but the grid search model\n",
    "# will use our predefined split internally to determine \n",
    "# which sample belongs to the validation set\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "print('best score {}'.format(grid_search.best_score_))\n",
    "print('best score {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15e01109",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = data_test['id']\n",
    "submission['label'] =grid_search.predict_proba(test_)[:,1]\n",
    "submission.to_csv('sample_submissiondec2_walkthrough.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c0f6d1",
   "metadata": {},
   "source": [
    "### Logistic regression model gives f1-score=0.84928 in leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fffdb4",
   "metadata": {},
   "source": [
    "# Trial 2\n",
    "#### i will change word vectorizer by char vectorizer with diff hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "604fd026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 40 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.7485954  0.7485921  0.74809574 0.74810075\n",
      " 0.74809134 0.74809728        nan        nan 0.74705803 0.74708712\n",
      " 0.74758828 0.74757442 0.7475885  0.74758723        nan        nan\n",
      " 0.7459945  0.74628367 0.74645504 0.74659362 0.74645596 0.74647053\n",
      "        nan        nan 0.74572645 0.74621383 0.74596602 0.74595889\n",
      " 0.74597491 0.74623584        nan        nan 0.74556362 0.74620904\n",
      " 0.74539894 0.74616934 0.74549787 0.7462071 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.7485954027682672\n",
      "best score {'classifier__C': 1.0, 'classifier__penalty': 'l1', 'classifier__solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "# firstly, make pipeline contains vectorizer and classifier\n",
    "pipe=Pipeline([\n",
    "    ('preprocessor',pipe_vect2),\n",
    "    ('classifier', LogisticRegression()),\n",
    "])\n",
    "C = [1.0, 10, 100, 1e3,1e5]\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "param_grid = {\n",
    "                  'classifier__C': C,\n",
    "                  'classifier__penalty': penalty,\n",
    "             'classifier__solver':['newton-cg', 'lbfgs', 'liblinear', 'saga']}\n",
    "\n",
    "# n_jobs means the cucurrent number of jobs\n",
    "# (on colab since we only have two cpu cores, we set it to 2)\n",
    "grid_search = GridSearchCV(\n",
    "    pipe, param_grid, cv=pds, verbose=1, n_jobs=2, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "# here we still use X_train; but the grid search model\n",
    "# will use our predefined split internally to determine \n",
    "# which sample belongs to the validation set\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "print('best score {}'.format(grid_search.best_score_))\n",
    "print('best score {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86840cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = data_test['id']\n",
    "submission['label'] =grid_search.predict_proba(test_)[:,1]\n",
    "submission.to_csv('sample_submissiondeclog char_walkthrough.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d621f9e1",
   "metadata": {},
   "source": [
    "### as i show ,when i used char vectorizer and make grid search ,the result is f1-score= 0.69924 which mean the word vectorizer is better than char vectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33bb2f0",
   "metadata": {},
   "source": [
    "## Trail 3 for XGBclassifier\n",
    "i try to use different classifier to see what happen \n",
    "-XGBclassifier with word vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "25efc8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:09:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:10:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:11:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:11:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "best score 0.8586988695274089\n",
      "best score OrderedDict([('classifier__learning_rate', 0.05), ('classifier__max_depth', 8), ('classifier__n_estimators', 600)])\n"
     ]
    }
   ],
   "source": [
    "# make pipeline with word vectorizer and classifier\n",
    "full_pipline = Pipeline([\n",
    "    ('preprocessor',pipe_vect1),\n",
    "    ('classifier', XGBClassifier()),\n",
    "])        \n",
    "#make bayes search\n",
    "bayes_search = BayesSearchCV(\n",
    "    full_pipline,\n",
    "    {\n",
    "     'classifier__max_depth':[1,4,5,7,8],\n",
    "    'classifier__n_estimators': [100, 200,300, 400,500,600],\n",
    "    'classifier__learning_rate': [0.1, 0.01, 0.05]}\n",
    ",\n",
    "    # number of trials \n",
    "    n_iter=3,\n",
    "    random_state=0,\n",
    "    verbose=1,\n",
    "    # we still use \n",
    "    cv=pds, scoring='roc_auc'\n",
    ")\n",
    "\n",
    "bayes_search.fit(x_train, y_train)\n",
    "\n",
    "print('best score {}'.format(bayes_search.best_score_))\n",
    "print('best score {}'.format(bayes_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b6db022",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = data_test['id']\n",
    "submission['label'] =bayes_search.predict_proba(test_)[:,1]\n",
    "submission.to_csv('sample_submissiondecxg_walkthrough.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f381bb68",
   "metadata": {},
   "source": [
    "### f1-score =0.81899 ,it's mean that the logistic regression with word vectorizer has more f1-score than XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686bd892",
   "metadata": {},
   "source": [
    "# Trial 4\n",
    "## Xgboost with different values of hyperparameters\n",
    "i will try to give largest values for hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38b0aeab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "C:\\Users\\Lenovo\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:00:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  7.8min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:08:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  7.1min finished\n",
      "C:\\Users\\Lenovo\\anaconda\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01:15:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "best score 0.8752546511853063\n",
      "best score OrderedDict([('classifier__learning_rate', 0.1), ('classifier__max_depth', 14), ('classifier__n_estimators', 800)])\n"
     ]
    }
   ],
   "source": [
    "# pipeline contains word vectorizer and classifier\n",
    "\n",
    "full_pipline = Pipeline([\n",
    "    ('preprocessor',pipe_vect1),\n",
    "    ('classifier', XGBClassifier()),\n",
    "])        \n",
    "bayes_search = BayesSearchCV(\n",
    "    full_pipline,\n",
    "    {\n",
    "     'classifier__max_depth':[10,12,13,14],\n",
    "    'classifier__n_estimators': [700,800,900],\n",
    "    'classifier__learning_rate': [0.1, 0.01, 0.05,0.02]}\n",
    ",\n",
    "    # number of trials \n",
    "    n_iter=2,\n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    "    # we still use \n",
    "    cv=pds, scoring='roc_auc'\n",
    ")\n",
    "\n",
    "bayes_search.fit(x_train, y_train)\n",
    "\n",
    "print('best score {}'.format(bayes_search.best_score_))\n",
    "print('best score {}'.format(bayes_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8113e9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = data_test['id']\n",
    "submission['label'] =bayes_search.predict_proba(test_)[:,1]\n",
    "submission.to_csv('sample_submissionxgb2_walkthrough.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d4834e",
   "metadata": {},
   "source": [
    "### f1-score has 0.83002 ,which mean that large values for hyperparameter are given better performance than small values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4de761f",
   "metadata": {},
   "source": [
    "# Trial 5\n",
    "## Xgboost with char vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "172b26a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:55:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:58:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:01:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "best score 0.7772293255715295\n",
      "best score OrderedDict([('classifier__learning_rate', 0.1), ('classifier__max_depth', 14), ('classifier__n_estimators', 800)])\n"
     ]
    }
   ],
   "source": [
    "# pipeline contains char vectorizer and classifier\n",
    "full_pipline = Pipeline([\n",
    "    ('preprocessor',pipe_vect2),\n",
    "    ('classifier', XGBClassifier()),\n",
    "])        \n",
    "bayes_search = BayesSearchCV(\n",
    "    full_pipline,\n",
    "    {\n",
    "     'classifier__max_depth':[10,12,13,14],\n",
    "    'classifier__n_estimators': [700,800,900],\n",
    "    'classifier__learning_rate': [0.1, 0.01, 0.05,0.02]}\n",
    ",\n",
    "    # number of trials \n",
    "    n_iter=2,\n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    "    # we still use \n",
    "    cv=pds, scoring='roc_auc'\n",
    ")\n",
    "\n",
    "bayes_search.fit(x_train, y_train)\n",
    "\n",
    "print('best score {}'.format(bayes_search.best_score_))\n",
    "print('best score {}'.format(bayes_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed875e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = data_test['id']\n",
    "submission['label'] =bayes_search.predict_proba(test_)[:,1]\n",
    "submission.to_csv('sample_submissionxgb2_walkthrough.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01869a93",
   "metadata": {},
   "source": [
    "## when i use char vectorizer the f1-score is decreased to 0.71939"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dd2c6b",
   "metadata": {},
   "source": [
    "# Trial 6\n",
    "## RandomCLassifier\n",
    "i will try diff classifier to see what is the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "638580ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 432 candidates, totalling 432 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=3)]: Done 194 tasks      | elapsed: 12.8min\n",
      "[Parallel(n_jobs=3)]: Done 432 out of 432 | elapsed: 42.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8507153249272552\n",
      "best score {'my_classifierran__max_depth': 60, 'my_classifierran__max_features': 'auto', 'my_classifierran__min_samples_leaf': 1, 'my_classifierran__min_samples_split': 2, 'my_classifierran__n_estimators': 300}\n"
     ]
    }
   ],
   "source": [
    "# pipeline contains word vectorizer and classifier\n",
    "pipe=Pipeline([\n",
    "    ('preprocessor',pipe_vect1),\n",
    "    ('my_classifierran', \n",
    "           RandomForestClassifier(random_state=42,class_weight = 'balanced'),)\n",
    "])        \n",
    "random_grid = {\n",
    "    'my_classifierran__max_features': ['auto', 'sqrt'],\n",
    "    'my_classifierran__n_estimators': [70,80,100,300],  \n",
    "     # my_classifier__n_estimators points to my_classifier->n_estimators \n",
    "    'my_classifierran__max_depth':[10, 20, 30, 40, 50, 60], \n",
    "    'my_classifierran__min_samples_split': [2, 5, 10], \n",
    "    'my_classifierran__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# cv=2 means two-fold cross-validation\n",
    "# n_jobs means the cucurrent number of jobs\n",
    "# (on colab since we only have two cpu cores, we set it to 2)\n",
    "random_search = GridSearchCV(\n",
    "    pipe, random_grid, cv=pds, verbose=1, n_jobs=3, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "random_search.fit(x_train, y_train)\n",
    "\n",
    "print('best score {}'.format(random_search.best_score_))\n",
    "print('best score {}'.format(random_search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44453f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = data_test['id']\n",
    "submission['label'] =random_search.predict_proba(test_)[:,1]\n",
    "submission.to_csv('sample_submissionrandom_walkthrough.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6180ab",
   "metadata": {},
   "source": [
    "### f1-score =0.81594 but still logistic regression is the best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50b7da6",
   "metadata": {},
   "source": [
    "# Trial 7\n",
    "## KNN classifier\n",
    "### try to make KNN with word vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a5d27cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\Lenovo\\anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\Lenovo\\anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\Lenovo\\anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\Lenovo\\anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n",
      "C:\\Users\\Lenovo\\anaconda\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8270945516572459\n",
      "best score OrderedDict([('my_classifier__n_neighbors', 29), ('my_classifier__weights', 'distance')])\n"
     ]
    }
   ],
   "source": [
    "pipe=Pipeline([('preprocessor',pipe_vect1),\n",
    "        ('my_classifier',KNeighborsClassifier())])\n",
    "k_range=list(range(1,30))\n",
    "#Creating parameter grid\n",
    "weights = ['uniform', 'distance']\n",
    "param={'my_classifier__n_neighbors':k_range\n",
    "       ,'my_classifier__weights':weights}\n",
    "search=BayesSearchCV(pipe,param,cv=pds,scoring='roc_auc')\n",
    "#fit model\n",
    "search.fit(x_train, y_train)\n",
    "\n",
    "print('best score {}'.format(search.best_score_))\n",
    "print('best score {}'.format(search.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db5c22b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make file submission\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = data_test['id']\n",
    "submission['label'] =search.predict_proba(test_)[:,1]\n",
    "submission.to_csv('sample_submissionknn_walkthrough.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318297ba",
   "metadata": {},
   "source": [
    "### f1-score =0.77568 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49030cf3",
   "metadata": {},
   "source": [
    "# Trial 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653e4be4",
   "metadata": {},
   "source": [
    "## Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "290923c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "best score 0.70488240233347\n",
      "best score OrderedDict([('my_classifier__criterion', 'gini'), ('my_classifier__max_depth', 14), ('my_classifier__min_samples_leaf', 100)])\n"
     ]
    }
   ],
   "source": [
    "# pipeline with word vectorizer\n",
    "pipe=Pipeline([('preprocessor',pipe_vect1),\n",
    "        ('my_classifier',DecisionTreeClassifier())])\n",
    "decision_search={\n",
    "    \"my_classifier__criterion\" : ['gini','entropy'],\n",
    "     \"my_classifier__max_depth\" : [10, 12,13,14],\n",
    "     \"my_classifier__min_samples_leaf\": [100,200,300,500,600,800]\n",
    " }\n",
    "#make bayessearch\n",
    "decision_search = BayesSearchCV(\n",
    "    pipe, decision_search, cv=pds, verbose=1, n_jobs=2, \n",
    "    scoring='roc_auc')\n",
    "# fit model\n",
    "decision_search.fit(x_train, y_train)\n",
    "print('best score {}'.format(decision_search.best_score_))\n",
    "print('best score {}'.format(decision_search.best_params_))\n",
    "#make file submission\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] = data_test['id']\n",
    "\n",
    "submission['label'] =decision_search.predict_proba(test_)[:,1]\n",
    "submission.to_csv('sample_submissiondec_walkthrough.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd03bf54",
   "metadata": {},
   "source": [
    "### f1-score =0.67215 , decision tree is the worst model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134cea20",
   "metadata": {},
   "source": [
    "# Trial 9\n",
    "## decision tree with char vectorizer\n",
    "i try to use char vectorizer to see if this model keeps the worst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1282a0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\skopt\\optimizer\\optimizer.py:449: UserWarning: The objective has been evaluated at this point before.\n",
      "  warnings.warn(\"The objective has been evaluated \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "Fitting 1 folds for each of 1 candidates, totalling 1 fits\n",
      "best score 0.6791013297399491\n",
      "best score OrderedDict([('my_classifier__criterion', 'entropy'), ('my_classifier__max_depth', 13), ('my_classifier__min_samples_leaf', 100)])\n"
     ]
    }
   ],
   "source": [
    "pipe=Pipeline([('preprocessor',pipe_vect2),\n",
    "        ('my_classifier',DecisionTreeClassifier())])\n",
    "decision_search={\n",
    "    \"my_classifier__criterion\" : ['gini','entropy'],\n",
    "     \"my_classifier__max_depth\" : [10, 12,13,14],\n",
    "     \"my_classifier__min_samples_leaf\": [100,200,300,500,600,800]\n",
    " }\n",
    "decision_search = BayesSearchCV(\n",
    "    pipe, decision_search, cv=pds, verbose=1, n_jobs=2, \n",
    "    scoring='roc_auc')\n",
    "\n",
    "decision_search.fit(x_train, y_train)\n",
    "print('best score {}'.format(decision_search.best_score_))\n",
    "print('best score {}'.format(decision_search.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9fe0d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] = data_test['id']\n",
    "\n",
    "submission['label'] =decision_search.predict_proba(test_)[:,1]\n",
    "submission.to_csv('sample_submissiondecchar_walkthrough.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3ae7b1",
   "metadata": {},
   "source": [
    "### f1-score =0.65622 , decision tree is the worst model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88ae761",
   "metadata": {},
   "source": [
    "# What is the difference between Character n-gram and Word n-gram? Which one tends to suffer more from the OOV issue?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b2ee38",
   "metadata": {},
   "source": [
    "-Represent unique word sequence of length n as feature                                     \n",
    "-Represent unique character sequence of length n as feature                                         \n",
    "-Character n-gram tends to suffer more from the OOV issue ,if a OOV word is present\n",
    "ten times in the corpus, it is counted ten times in the\n",
    "character n-gram). Note that it is also possible to include\n",
    "more data from other sources to enrich the character-level\n",
    "language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eec6285",
   "metadata": {},
   "source": [
    "# What is the difference between stop word removal and stemming? Are these techniques language-dependent?\n",
    "-stopword removal is removing the words that occur commonly across all the documents. Typically, articles and pronouns are generally classified as stop words\n",
    "-stemming is the process of removing a part of a word, or reducing a word to its stem or root.            \n",
    "-stemming and stopword are dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b10b00",
   "metadata": {},
   "source": [
    "# Is tokenization techniques language dependent? Why?\n",
    "Yes it's dependent because tokenization is spliting the raw text into small token or single word. These tokens help in understanding the context or developing the model for the NLP and helps in interpreting the meaning of the text by analyzing the sequence of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06b5953",
   "metadata": {},
   "source": [
    "## What is the difference between count vectorizer and tf-idf vectorizer? Would it be feasible to use all possible n-grams? If not, how should you select them?\n",
    "tf-idf vectorizer returns floats, not only focuses on the frequency of words present in the corpus but also provides the importance of the words,while the count vectorizer returns integers ,converts a given set of strings into a frequency representation .\n",
    "No, because the best N-grams are chosen by tuning and testing the previous n-grams extraction query on different hyperparameter values, including different sentences examples and N-gram size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98f23ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
